{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task-01**"
      ],
      "metadata": {
        "id": "5MjHrvjI6d9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtcDzc0I6Nyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb949e1-d39f-4702-d498-fb4874901fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1025, 14)\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
            "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
            "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
            "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
            "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   2     3       0  \n",
            "1   0     3       0  \n",
            "2   0     3       0  \n",
            "3   1     3       0  \n",
            "4   3     2       0  \n",
            "\n",
            "Missing values per column:\n",
            " age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          0\n",
            "thal        0\n",
            "target      0\n",
            "dtype: int64\n",
            "\n",
            "Training Logistic Regression with penalty = l1\n",
            "Training Accuracy: 0.8647, Testing Accuracy: 0.8182\n",
            "\n",
            "Training Logistic Regression with penalty = l2\n",
            "Training Accuracy: 0.8647, Testing Accuracy: 0.8182\n",
            "\n",
            "Training Logistic Regression with penalty = elasticnet\n",
            "Training Accuracy: 0.8647, Testing Accuracy: 0.8182\n",
            "\n",
            "Comparison of Different Penalties:\n",
            "      Penalty  Training Accuracy  Testing Accuracy\n",
            "0          l1           0.864714          0.818182\n",
            "1          l2           0.864714          0.818182\n",
            "2  elasticnet           0.864714          0.818182\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "heart_df = pd.read_csv('heart.csv')\n",
        "\n",
        "print(\"Dataset shape:\", heart_df.shape)\n",
        "print(heart_df.head())\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\\n\", heart_df.isnull().sum())\n",
        "\n",
        "# Features and target\n",
        "X = heart_df.drop(\"target\", axis=1)\n",
        "y = heart_df[\"target\"].astype(int)  # convert to integer for classification\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "penalties = [\"l1\", \"l2\", \"elasticnet\"]\n",
        "results = []\n",
        "\n",
        "for pen in penalties:\n",
        "    print(f\"\\nTraining Logistic Regression with penalty = {pen}\")\n",
        "\n",
        "    # L1 and ElasticNet require solver='saga'\n",
        "    if pen in [\"l1\", \"elasticnet\"]:\n",
        "        solver = \"saga\"\n",
        "    else:\n",
        "        solver = \"lbfgs\"  # default for l2\n",
        "\n",
        "    # ElasticNet requires l1_ratio parameter\n",
        "    if pen == \"elasticnet\":\n",
        "        model = LogisticRegression(\n",
        "            penalty=pen,\n",
        "            solver=solver,\n",
        "            l1_ratio=0.5,\n",
        "            max_iter=5000,\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        model = LogisticRegression(\n",
        "            penalty=pen,\n",
        "            solver=solver,\n",
        "            max_iter=5000,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    results.append([pen, train_acc, test_acc])\n",
        "    print(f\"Training Accuracy: {train_acc:.4f}, Testing Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Create a results dataframe\n",
        "df_results = pd.DataFrame(results, columns=[\"Penalty\", \"Training Accuracy\", \"Testing Accuracy\"])\n",
        "print(\"\\nComparison of Different Penalties:\")\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Notes / Observations**\n",
        "- L1 and ElasticNet require solver='saga'.\n",
        "- ElasticNet requires l1_ratio parameter to combine L1 and L2 penalties.\n",
        "- lbfgs solver works only with l2 penalty.\n",
        "- max_iter is increased to 5000 to ensure convergence.\n"
      ],
      "metadata": {
        "id": "ifV6NPvM6q8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK - 02**"
      ],
      "metadata": {
        "id": "Fx6h1qoR9VHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_iris_scaled = scaler.fit_transform(X_iris)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_iris_scaled, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "solvers = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
        "solver_results = []\n",
        "\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=5000, multi_class='auto', random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "        test_acc = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "        solver_results.append([solver, train_acc, test_acc])\n",
        "\n",
        "    except Exception as e:\n",
        "        solver_results.append([solver, None, None])\n",
        "        print(f\"Solver {solver} encountered an error: {e}\")\n",
        "\n",
        "df_solver_results = pd.DataFrame(\n",
        "    solver_results, columns=[\"Solver\", \"Training Accuracy\", \"Testing Accuracy\"]\n",
        ")\n",
        "\n",
        "print(\"\\nComparison of Solvers on Iris Dataset:\")\n",
        "print(df_solver_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WlGtKb69bUw",
        "outputId": "bfebfc10-b238-4dca-be80-7a78d304a118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of Solvers on Iris Dataset:\n",
            "            Solver  Training Accuracy  Testing Accuracy\n",
            "0            lbfgs           0.980952          0.911111\n",
            "1        liblinear           0.933333          0.800000\n",
            "2        newton-cg           0.980952          0.911111\n",
            "3  newton-cholesky           0.980952          0.911111\n",
            "4              sag           0.980952          0.911111\n",
            "5             saga           0.980952          0.911111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The solver affects convergence speed and sometimes accuracy.\n",
        "'liblinear' is good for small datasets, but does not support all multi-class options.\n",
        "'lbfgs', 'newton-cg', and 'saga' are robust for small to medium datasets and multi-class problems.\n",
        "'sag' and 'saga' are faster for large datasets.\n",
        "In this case (Iris dataset is small), lbfgs or newton-cg gives slightly better stability and accuracy."
      ],
      "metadata": {
        "id": "kMgfljvk-LIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: For small datasets, lbfgs or newton-cg is stable and performs well."
      ],
      "metadata": {
        "id": "ffS1jaj4_aMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heart_df = pd.read_csv('heart.csv')\n",
        "\n",
        "# Features and target\n",
        "X_heart = heart_df.drop(\"target\", axis=1)\n",
        "y_heart = heart_df[\"target\"].astype(int)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_heart_scaled = scaler.fit_transform(X_heart)\n",
        "\n",
        "# Train-test split\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
        "    X_heart_scaled, y_heart, test_size=0.3, random_state=42, stratify=y_heart\n",
        ")\n",
        "\n",
        "heart_solver_results = []\n",
        "\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=5000, random_state=42)\n",
        "        model.fit(X_train_h, y_train_h)\n",
        "        train_acc = accuracy_score(y_train_h, model.predict(X_train_h))\n",
        "        test_acc = accuracy_score(y_test_h, model.predict(X_test_h))\n",
        "        heart_solver_results.append([solver, train_acc, test_acc])\n",
        "    except Exception as e:\n",
        "        heart_solver_results.append([solver, None, None])\n",
        "        print(f\"Solver {solver} error on Heart dataset: {e}\")\n",
        "\n",
        "df_heart_results = pd.DataFrame(\n",
        "    heart_solver_results, columns=[\"Solver\", \"Training Accuracy\", \"Testing Accuracy\"]\n",
        ")\n",
        "\n",
        "print(\"\\nComparison of Solvers on Heart Disease Dataset:\")\n",
        "print(df_heart_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZx_bTJi-RZG",
        "outputId": "37e4bb27-e3d3-46b4-8436-96e4a7fc193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of Solvers on Heart Disease Dataset:\n",
            "            Solver  Training Accuracy  Testing Accuracy\n",
            "0            lbfgs           0.864714          0.818182\n",
            "1        liblinear           0.864714          0.818182\n",
            "2        newton-cg           0.864714          0.818182\n",
            "3  newton-cholesky           0.864714          0.818182\n",
            "4              sag           0.864714          0.818182\n",
            "5             saga           0.864714          0.818182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heart Disease dataset is larger than Iris dataset (~303 rows vs 150 rows).\n",
        "Solvers like 'sag' and 'saga' are optimized for larger datasets and often converge faster.\n",
        "Small datasets like Iris work well with 'lbfgs' and 'newton-cg'.\n",
        "Accuracy difference between solvers is minor for small datasets but can affect convergence time for larger datasets.\n",
        "In Heart Disease dataset, solver choice slightly affects training time; accuracy remains similar for most solvers except liblinear may converge slower for larger data."
      ],
      "metadata": {
        "id": "rXB6EOlB-vqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: Dataset is not large enough to show solver differences. All solvers converge to similar solution."
      ],
      "metadata": {
        "id": "63E4cwqG_VzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Briefly discuss the effect of solver on your dataset:\n",
        "The solver in logistic regression decides how the model finds the best coefficients for predicting the target. On the Iris dataset, which is small, most solvers like lbfgs, newton-cg, and saga gave very high accuracy, but liblinear was a bit lower because it uses a one-vs-rest method for multi-class problems, which is not as stable for very small datasets. On the Heart Disease dataset, which is a medium-sized binary dataset, all solvers gave almost the same accuracy. This shows that for small to medium datasets, solver choice does not change accuracy much, but it can affect how fast the model learns.\n",
        "\n",
        "2. Have you found the similarity as mentioned by Sklearn that which solver is best for small, medium or larger dataset:\n",
        "Yes, the results are similar to what Sklearn recommends. For small datasets like Iris, solvers such as lbfgs, newton-cg, and newton-cholesky work well because they are stable and handle multi-class problems. Solvers like sag and saga are designed for larger datasets and use stochastic methods to converge faster. Our Heart Disease dataset is a medium-sized dataset, and all solvers gave similar results, which matches Sklearnâ€™s explanation that solver mainly affects speed for bigger datasets but not accuracy for small or medium datasets.\n",
        "\n",
        "3. Which solver is best in your case and why:\n",
        "For the Iris dataset, lbfgs or newton-cg is the best because they gave the highest accuracy and are stable for multi-class problems on small datasets. For the Heart Disease dataset, which is slightly larger, lbfgs or saga works well because they converge reliably and handle binary classification without issues. In general, the best solver depends on the size and type of dataset, but for these datasets, lbfgs is a safe and reliable choice.\n",
        "\n",
        "4. Now copy this file and apply a new dataset (Heart Disease) and compare, does it really get affected by the size of dataset:\n",
        "When we applied the same solvers to the Heart Disease dataset, which is bigger than Iris, we saw that accuracy stayed almost the same for all solvers. This shows that for small to medium datasets, the solver does not really affect accuracy much. What changes more is how fast the model reaches the final solution. For very large datasets, solvers like sag and saga would be better because they can learn faster without using too much memory. So yes, the dataset size can affect which solver is faster, but for these datasets, accuracy was not affected by the size."
      ],
      "metadata": {
        "id": "4Gf-WVR8AFEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK-03**"
      ],
      "metadata": {
        "id": "FJLCjm6RBHYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# Logistic Regression\n",
        "lr_model = LogisticRegression(solver='lbfgs', max_iter=5000, multi_class='auto', random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_train_acc = accuracy_score(y_train, lr_model.predict(X_train))\n",
        "lr_test_acc = accuracy_score(y_test, lr_model.predict(X_test))\n",
        "\n",
        "# Perceptron\n",
        "perceptron_model = Perceptron(max_iter=5000, tol=1e-3, random_state=42)\n",
        "perceptron_model.fit(X_train, y_train)\n",
        "perceptron_train_acc = accuracy_score(y_train, perceptron_model.predict(X_train))\n",
        "perceptron_test_acc = accuracy_score(y_test, perceptron_model.predict(X_test))\n",
        "\n",
        "# Comparison\n",
        "comparison = pd.DataFrame({\n",
        "    \"Model\": [\"Logistic Regression\", \"Perceptron\"],\n",
        "    \"Training Accuracy\": [lr_train_acc, perceptron_train_acc],\n",
        "    \"Testing Accuracy\": [lr_test_acc, perceptron_test_acc]\n",
        "})\n",
        "\n",
        "print(\"\\nComparison of Logistic Regression vs Perceptron:\")\n",
        "print(comparison)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbglo62MBPoG",
        "outputId": "8d57527c-4c03-42ed-d0ae-daf8c17727a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of Logistic Regression vs Perceptron:\n",
            "                 Model  Training Accuracy  Testing Accuracy\n",
            "0  Logistic Regression           0.980952          0.911111\n",
            "1           Perceptron           0.895238          0.844444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron and Logistic Regression are both used for classification, but they work differently. A Perceptron is a simple model that makes yes or no predictions by calculating a weighted sum of the inputs and applying a step function, so it cannot give probabilities and only works well if the classes are linearly separable. Logistic Regression also uses a weighted sum of inputs but passes it through a sigmoid function to output a probability between 0 and 1, which allows it to handle overlapping classes better. Logistic Regression uses gradient-based methods to find the best weights and can include regularization to prevent overfitting, making it more robust than the Perceptron.\n"
      ],
      "metadata": {
        "id": "P-HRwXetBoJA"
      }
    }
  ]
}